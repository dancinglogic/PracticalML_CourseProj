---
title: "PracticalML_CourseProj"
author: "Jessica"
date: "July 20, 2015"
output: html_document
---

## Background ##

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.

In this project we use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to classify the quality of their barbell lifts. Participants were asked to perform barbell lifts correctly (Class A) and to simulate four common mistakes (Classes B, C, D, and E).

More information is available <a href="http://groupware.les.inf.puc-rio.br/har">here</a> (see the section on the Weight Lifting Exercise Dataset). 

## Getting and Cleaning Data ##

All machine-learning work was performed using the caret and randomForest R packages.
We used the doParallel package to speed things up.
```{r libraries, message=FALSE, results='hide'}
library(caret)
library(randomForest)
library(doParallel)
registerDoParallel(cores=16)
```


The training and testing data sets were obtained from the URLs specified in the course project.
```{r loaddata, cache=TRUE, results='hide'}
# Get training and test data
if(!file.exists("train.csv")){
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
              destfile="train.csv", method="curl")
}
if(!file.exists("test.csv")){ 
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
              destfile="test.csv", method="curl")
}
training = read.table("train.csv", row.names=1, sep=",", header=TRUE)
testing = read.table("test.csv", row.names=1, sep=",", header=TRUE)
```

The training data has 158 variables (and one class label). That's kind of a lot of variables, so let's try to
cut it down some.
```{r}
dim(training)
```

There are some columns that mostly consist of NAs. It appears that all columns are either 100% non-NA values, or about 98% NA values. Let's drop columns that are ~98% NA values because those probably aren't very useful for classification (equivalently, only keep columns that are almost 100% non-NA values).
```{r cache=TRUE}
# Calculate what percent of each column consists of actual values as opposed to NA
percentOK <- sapply(names(training), function(x) {sum(! is.na(training[x])) / dim(training)[1]})
unique(percentOK)
# Either the column is all OK, or it's only 2% OK values. Only keep columns that are all OK.
columnstokeep <- (percentOK > 0.9)
sum(columnstokeep)
# Looks like we're keeping 92 columns.
training_92 <- subset(training, select=columnstokeep)
sum(complete.cases(training_92)) == dim(training_92)[1]
# Now all the NAs are gone.
```

There are still columns with values we can't use. Conveniently, these are
columns that have close to zero variance anyway, so we can probably leave them out
of the model-building with no ill effects.
```{r cache=TRUE}
# There are still columns with #DIV/0! here and there
sum(training_92=="#DIV/0!")
# Get rid of columns that don't look very useful, statistically speaking
nearZeroVarCols <- nearZeroVar(training_92)
length(nearZeroVarCols)
training_58 <- subset(training_92, select=-nearZeroVarCols)
sum(training_58=="#DIV/0!")
# Excellent. That appears to have gotten rid of the #DIV/0! cases.
```

Finally, get rid of the features such as participant names and timestamps since those don't make sense as predictors if the goal is to 
make predictions using the accelerometer data.
```{r}
training_reduced <- training_58[, 6:58]
```

## Cross-Validation and Model Fitting##

We use a random forest model because those consistently perform well.
We split the data into features and labels to give to the model, rather than using a formula, because it appears that using a formula makes the model-building noticeably slower.
```{r cache=TRUE}
# Set up input to random forest methods
features <- subset(training_reduced, select = -classe)
labels <- training_reduced$classe
# Get the model, or generate it if we don't have it.
if (file.exists("modelfit.rds")) {
  modelfit <- readRDS("modelfit.rds")
} else {
    set.seed(42)
    modelfit = train(features, labels, method="rf", ntree=500)
    saveRDS(modelfit, "modelfit.rds")
}
```

When it comes to cross-validation, random forests are a bit of a special case. There is cross-validation going on, but it's hiding within the "train" method.

According to <a href=
"https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr">Breiman</a>
there is no need to perform separate cross-validation when using random forests to get an estimate of the out-of-bag (oob) error. Essentially, cross-validation is being performed within the train() method that builds the random forest model. To build each tree a sample is taken from the original data. Part of this sample is used to build the tree, and the rest of the sample is used to estimate the error.

We expect the out-of-bag error to be


Finally, we predict the values for the testing set. After submitting we found that everything was predicted correctly.
```{r cache=TRUE}
# Restrict the testing data to the variables we used in training.
featurenames <- names(training_reduced)[names(training_reduced)!= "classe"]
testing_reduced <- subset(testing, select=featurenames)
# Predict on the training data
pred <- predict(modelfit, newdata=testing_reduced)
# Function provided in project description
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
# Write out results using provided function
pml_write_files(as.character(pred))
```
